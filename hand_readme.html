<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AirExo 工具使用指南</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        h1, h2, h3 {
            color: #0056b3;
            border-bottom: 2px solid #0056b3;
            padding-bottom: 10px;
        }
        h1 {
            text-align: center;
            font-size: 2.5em;
        }
        h2 {
            font-size: 2em;
            margin-top: 40px;
        }
        h3 {
            font-size: 1.5em;
            margin-top: 30px;
            border-bottom: 1px solid #ccc;
        }
        p, li {
            font-size: 1.1em;
        }
        code {
            font-family: "Courier New", Courier, monospace;
            background-color: #e9ecef;
            padding: 2px 5px;
            border-radius: 4px;
            font-size: 0.95em;
        }
        pre {
            background-color: #212529;
            color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .note {
            background-color: #fff3cd;
            border-left: 5px solid #ffeeba;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .path {
            color: #28a745;
        }
        .command {
            color: #17a2b8;
        }
        ul {
            list-style-type: none;
            padding-left: 0;
        }
        li {
            margin-bottom: 10px;
        }
        strong {
            color: #d9534f;
        }
    </style>
</head>
<body>

    <h1>AirExo 工具使用指南</h1>

    <h2>SAM2 使用流程</h2>
    <p>本节概述了使用 SAM2 工具在视频帧中选择和移除对象的流程。</p>

    <h3>第一步：将视频分割成图像帧</h3>
    <p>使用 <code>ffmpeg</code> 命令从您的视频中提取帧。</p>
    <p><strong>从 1 开始编号的命令：</strong></p>
    <pre>ffmpeg -i input_video.mp4 -vf "fps=30" <code class="path">/home/tracy/airexo/AirExo-2/data/train/scene_0036/color/%d.png</code></pre>
    <ul>
        <li><code>input_video.mp4</code>: 您的源视频文件。</li>
        <li><code>fps=30</code>: 提取的帧率。</li>
        <li><code class="path">/home/tracy/airexo/AirExo-2/data/train/scene_0036/color/%d.png</code>: 图像帧的输出目录。</li>
    </ul>

    <p><strong>从 0 开始编号的命令：</strong></p>
    <pre>ffmpeg -start_number 0 -i input_video.mp4 -vf "fps=30" <code class="path">/home/tracy/airexo/AirExo-2/data/train/scene_0036/color/%d.png</code></pre>

    <div class="note">
        <strong>重要提示：</strong>请确保输出文件夹中只包含提取的图像，而不包含原始视频文件。
    </div>
    
    <h3>第二步：运行 SAM2 并进行配置</h3>
    <ol>
        <li>导航到 AirExo 项目中的 <code>/utils/sam2/</code> 目录。</li>
        <li>执行 <code>main.py</code> 脚本。</li>
        <li>程序会首先提示您输入图像帧的总数。</li>
        <li>稍等片刻后，将显示第一帧。点击您希望移除的区域，一个绿色的框将自动出现在选区周围。</li>
        <li>按 <strong>Enter</strong> 键或关闭图像窗口，即可开始自动处理所有帧。</li>
    </ol>
    <p>请记得在相应的 <code>default.yaml</code> 文件中配置输出路径。您还需要根据需要通过修改 <code>start_frame: 0</code> 或 <code>start_frame: 1</code> 来设置起始帧号。</p>


    <h2>Propainter 使用流程</h2>
    <p>请按照以下步骤使用 Propainter 进行视频修复。</p>
    
    <h3>执行：</h3>
    <p>运行位于 <code class="path">/airexo/adaptor/</code> 目录下的 <code>inpainting.py</code> 脚本。完成后，最终的视频路径将输出在终端中。</p>

    <h3>配置：</h3>
    <p>在位于 <code class="path">/airexo/configs/</code> 的 <code>inpainting.yaml</code> 文件中调整参数，以管理视频质量并减少 GPU 显存占用。</p>
    <pre>
video:
  resize_ratio: 0.5  # 用于处理视频的缩放比例 (例如, 0.5)
	
processing:  
  neighbor_length: 5 # 局部相邻帧的长度 (例如, 5)
  subvideo_length: 40 # 处理长视频时子视频的长度 (例如, 40)
	
runtime:
  fp16: true  # 推理时使用半精度以节省显存
    </pre>


    <h2>ControlNet 使用流程</h2>
    <p>本节详细介绍使用 ControlNet 进行训练和推理的流程。</p>
    
    <h3>关键脚本：</h3>
    <ul>
        <li><strong>推理：</strong>使用 <code class="path">/home/tracy/airexo/AirExo-2/airexo/adaptor/controlnet_inference.py</code> 将训练好的 ControlNet 模型应用于新图像。</li>
        <li><strong>训练：</strong>使用 <code class="path">/home/tracy/airexo/AirExo-2/utils/controlnet/train.py</code> 在数据集上训练 ControlNet 模型。</li>
    </ul>

    <h3>训练流程：</h3>
    <p>在使用 <code>train.py</code> 开始训练之前，您必须在您的虚拟环境中设置网络代理。</p>
    <ol>
        <li><strong>设置代理环境变量：</strong></li>
<pre>
export http_proxy="http://127.0.0.1:7890"
export https_proxy="http://127.0.0.1:7890"
export all_proxy="socks5://127.0.0.1:7890"
</pre>
        <li><strong>验证网络连接：</strong></li>
        <p>运行以下命令以检查您是否能成功连接到 Hugging Face。</p>
<pre>
curl -I https://huggingface.co
</pre>
        <li>一旦连接被验证，您就可以继续进行训练。</li>
    </ol>

    <h2>Robot 相关配置</h2>

    <h3>Step1</h3>
    <p>将 <code class="path">airexo/urdf_models</code> 文件夹替换原来项目中的文件夹。</p>

    <h3>Step2</h3>
    <p>将 <code class="path">configs/tests/urdf/robot.yaml</code> 中的 <code>urdf_file</code> 路径改为：
    <code class="path">airexo/urdf_models/robot/true_robot.urdf</code></p>

    <h3>Step3</h3>
    <p>将 <code class="path">airexo/tests/urdf_robot.py</code> 文件内容替换为如下代码：</p>
    <pre>
import os
import hydra
import numpy as np
import open3d as o3d

from omegaconf import OmegaConf

from airexo.helpers.constants import *
from airexo.helpers.urdf_robot import forward_kinematic

# 省略 main() 和运行部分，仅保留代码主体
OmegaConf.resolve(cfg)

cur_transforms, visuals_map = forward_kinematic(
    left_joint = cfg.left_joint, 
    right_joint = cfg.right_joint,
    left_joint_cfgs = cfg.robot_left_joint_cfgs,
    right_joint_cfgs = cfg.robot_right_joint_cfgs,
    is_rad = False,
    urdf_file = cfg.urdf_file,
    with_visuals_map = True
)

visualizer = o3d.visualization.Visualizer()
visualizer.create_window(width = 1280, height = 720)

frame = o3d.geometry.TriangleMesh.create_coordinate_frame(0.2).transform(ROBOT_PREDEFINED_TRANSFORMATION)
visualizer.add_geometry(frame)

model_meshes = {}
for link, transform in cur_transforms.items():
    model_meshes[link] = {}
    for v in visuals_map[link]:
        if v.geom_param is None:
            continue

        tf = ROBOT_PREDEFINED_TRANSFORMATION @ transform.matrix() @ v.offset.matrix()

        if isinstance(v.geom_param, str):
            mesh_path = os.path.join(os.path.dirname(cfg.urdf_file), v.geom_param)
            mesh = o3d.io.read_triangle_mesh(mesh_path)

        elif isinstance(v.geom_param, tuple):
            if len(v.geom_param) == 3 and v.geom_param[0] == 'cylinder':
                length = v.geom_param[1]
                radius = v.geom_param[2]
                mesh = o3d.geometry.TriangleMesh.create_cylinder(radius=radius, height=length)

            elif len(v.geom_param) == 2 and all(isinstance(x, (int, float)) for x in v.geom_param):
                length, radius = v.geom_param
                mesh = o3d.geometry.TriangleMesh.create_cylinder(radius=radius, height=length)

            else:
                print(f"Unknown geometry param for link {link}: {v.geom_param}")
                continue
        else:
            print(f"Unknown geometry param type for link {link}: {v.geom_param}")
            continue

        mesh.transform(tf)
        mesh.compute_vertex_normals()
        model_meshes[link][v.geom_param] = mesh
        visualizer.add_geometry(mesh, reset_bounding_box=True)

visualizer.run()
    </pre>

    <h3>Step4</h3>
    <p>进入 airexo 虚拟环境后运行：</p>
    <pre>python -m airexo.tests.urdf_robot</pre>

    <h2>Robot 视角调整</h2>
    <p>
        本节介绍如何进行robot视角调整的相关设置和配置。目前我们先通过Meshcat进行可视化，
        但由于其视角调整貌似有局限性，会自动朝向robot的方向，所以我们采取了两次"镜像"的方法，
        在设置相机视角时将相机前方（Z轴）翻转180°，效果相当于一次镜像，录制视频后，对视频再做一次
        左右的镜像使视角恢复正常。
    
    </p>

    <h3>1.环境依赖安装</h3>
    <p>在开始robot视角调整之前，需要先安装必要的库依赖。进入虚拟环境后运行以下命令：</p>
    
    <h4>安装moviepy和imageio库：</h4>
    <pre>pip install moviepy imageio[ffmpeg]</pre>
    
    <h4>升级imageio库：</h4>
    <pre>pip install --upgrade imageio</pre>
    
    <h4>升级imageio-ffmpeg库：</h4>
    <pre>pip install --upgrade imageio-ffmpeg</pre>

    <div class="note">
        <strong>注意：</strong>确保在正确的虚拟环境中执行这些安装命令，以避免依赖冲突。
    </div>

    <h3>2.设置视角函数以及应用</h3>

    <p> 只需要插入一下函数:</p>
    <pre> 

 def look_at(viz, camera_pos, target_pos, up=np.array([0, 0, 1])):
    camera_pos = np.array(camera_pos)
    target_pos = np.array(target_pos)
    
    # 设置焦点
    T = np.eye(4)
    T[:3, 3] = target_pos
    viz.viewer["/Cameras/default"].set_transform(T)

    # 相机相对焦点的偏移
    offset = camera_pos - target_pos

    viz.viewer["/Cameras/default/rotated/&ltobject&gt"].set_property("position", offset.tolist())

    # ---- 关键：修正旋转，使相机真的看向目标 ----
    # forward = -Z 方向
    forward = (target_pos - camera_pos)
    forward /= np.linalg.norm(forward)

    right = np.cross(up, forward)
    right /= np.linalg.norm(right)

    true_up = np.cross(forward, right)


    R = np.eye(4)
    R[:3, 0] = right
    R[:3, 1] = true_up
    R[:3, 2] = -forward   # 注意这里是 +Z/-Z，可能需要 flip


    viz.viewer["/Cameras/default/rotated"].set_transform(R)
    </pre>

    <p> 之后再main函数中调用look_at函数即可:</p>

    <pre>
        # 可以放在vis.display的后面
        look_at(viz,camera_pos=[-0.0032, 0.01, 1.9526],target_pos=[-0.0032, -0.5903, 1.3026])
    </pre>

    <h3>3.视频的镜像处理</h3>
    只需要把对应需要处理的视频路径配置到input_video参数就好：

    <pre>
from moviepy.editor import VideoFileClip, vfx

def mirror_video(input_path, output_path):
    """
    对视频进行左右镜像处理
    :param input_path: 输入视频路径
    :param output_path: 输出视频路径
    """
    # 读取视频 - 直接使用VideoFileClip
    clip = VideoFileClip(input_path)

    # 左右镜像（水平翻转） - 使用正确的语法
    flipped_clip = clip.fx(vfx.mirror_x)

    # 输出视频
    flipped_clip.write_videofile(
        output_path,
        codec="libx264",
        audio_codec="aac",
        fps=clip.fps
    )
    
    # 释放资源
    clip.close()
    flipped_clip.close()

if __name__ == "__main__":
    input_video = "mirror_wild.mkv"        # 输入视频文件
    output_video = "mirror_ans.mp4"  # 输出视频文件
    mirror_video(input_video, output_video)
    print("左右镜像处理完成！")
    </pre>
</body>
</html>